<?xml version="1.0" encoding="UTF-8"?>
<doc:document xmlns:doc="http://www.elsevier.com/xml/document/schema"
              xmlns:dp="http://www.elsevier.com/xml/common/doc-properties/schema"
              xmlns:cps="http://www.elsevier.com/xml/common/consyn-properties/schema"
              xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
              xmlns:dct="http://purl.org/dc/terms/"
              xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/"
              xmlns:oa="http://vtw.elsevier.com/data/ns/properties/OpenAccess-1/"
              xmlns:bam="http://vtw.elsevier.com/data/voc/ns/bam-vtw-1/"
              xmlns:cp="http://vtw.elsevier.com/data/ns/properties/Copyright-1/"
              xmlns:cja="http://www.elsevier.com/xml/cja/schema"
              xmlns:ja="http://www.elsevier.com/xml/ja/schema"
              xmlns:bk="http://www.elsevier.com/xml/bk/schema"
              xmlns:ce="http://www.elsevier.com/xml/common/schema"
              xmlns:mml="http://www.w3.org/1998/Math/MathML"
              xmlns:cals="http://www.elsevier.com/xml/common/cals/schema"
              xmlns:tb="http://www.elsevier.com/xml/common/table/schema"
              xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/schema"
              xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/schema"
              xmlns:xlink="http://www.w3.org/1999/xlink">
   <rdf:RDF>
      <rdf:Description rdf:about="http://dx.doi.org/10.1016/j.dib.2022.107791">
         <dct:format>application/xml</dct:format>
         <dct:title>A pixel-wise annotated dataset of small overlooked indoor objects for semantic segmentation applications.</dct:title>
         <dct:creator>Elhassan Mohamed</dct:creator>
         <dct:creator>Konstantinos Sirlantzis</dct:creator>
         <dct:creator>Gareth Howells</dct:creator>
         <dct:subject>
            <rdf:Bag>
               <rdf:li>Semantic segmentation</rdf:li>
               <rdf:li>Indoor objects</rdf:li>
               <rdf:li>Door handles</rdf:li>
               <rdf:li>Image dataset</rdf:li>
               <rdf:li>Deep learning</rdf:li>
               <rdf:li>Pixels classification</rdf:li>
               <rdf:li>Convolutional neural network</rdf:li>
            </rdf:Bag>
         </dct:subject>
         <dct:description>Data in Brief 40 (2022). doi:10.1016/j.dib.2022.107791</dct:description>
         <prism:aggregationType>journal</prism:aggregationType>
         <prism:publicationName>Data in Brief</prism:publicationName>
         <prism:copyright>Crown Copyright © 2022 Published by Elsevier Inc.</prism:copyright>
         <dct:publisher>Elsevier Inc.</dct:publisher>
         <prism:issn>2352-3409</prism:issn>
         <prism:volume>40</prism:volume>
         <prism:coverDisplayDate>February 2022</prism:coverDisplayDate>
         <prism:doi>10.1016/j.dib.2022.107791</prism:doi>
         <prism:url>http://dx.doi.org/10.1016/j.dib.2022.107791</prism:url>
         <dct:identifier>doi:10.1016/j.dib.2022.107791</dct:identifier>
         <bam:articleNumber>107791</bam:articleNumber>
         <oa:openAccessInformation>
            <oa:openAccessStatus xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">http://vtw.elsevier.com/data/voc/oa/OpenAccessStatus#Full</oa:openAccessStatus>
            <oa:openAccessEffective xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">2022-01-04T10:39:36Z</oa:openAccessEffective>
            <oa:sponsor xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
               <oa:sponsorType>http://vtw.elsevier.com/data/voc/oa/SponsorType#Author</oa:sponsorType>
            </oa:sponsor>
            <oa:userLicense xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">http://creativecommons.org/licenses/by/4.0/</oa:userLicense>
         </oa:openAccessInformation>
         <cp:licenseLine xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">This is an open access article under the CC BY license.</cp:licenseLine>
      </rdf:Description>
   </rdf:RDF>
   <dp:document-properties>
      <dp:aggregation-type>Journals</dp:aggregation-type>
      <dp:version-number>S300.2</dp:version-number>
   </dp:document-properties>
   <ja:article docsubtype="dat" version="5.6" xml:lang="en">
      <ja:item-info>
         <ja:jid>DIB</ja:jid>
         <ja:aid>107791</ja:aid>
         <ce:article-number>107791</ce:article-number>
         <ce:pii>S2352-3409(22)00003-8</ce:pii>
         <ce:doi>10.1016/j.dib.2022.107791</ce:doi>
         <ce:copyright type="unknown" year="2022"/>
      </ja:item-info>
      <ja:head>
         <ce:dochead id="dchd0001">
            <ce:textfn id="dhtxt0001">Data Article</ce:textfn>
         </ce:dochead>
         <ce:title id="tte0002">A pixel-wise annotated dataset of small overlooked indoor objects for semantic segmentation applications.</ce:title>
         <ce:author-group id="aut0001">
            <ce:author id="au0001"
                       author-id="S2352340922000038-a3607bc716308b36df2eda1f14dcbea0"
                       orcid="0000-0001-9746-1564">
               <ce:given-name>Elhassan</ce:given-name>
               <ce:surname>Mohamed</ce:surname>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/conceptualization">Conceptualization</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/methodology">Methodology</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/visualization">Visualization</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/formal-analysis">Formal analysis</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/writing-original-draft">Writing – original draft</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/writing-review-editing">Writing – review &amp; editing</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/data-curation">Data curation</ce:contributor-role>
               <ce:cross-ref refid="cor0001">
                  <ce:sup loc="post">⁎</ce:sup>
               </ce:cross-ref>
               <ce:e-address id="ead0001" type="email" xlink:href="mailto:enrm4@kent.ac.uk">enrm4@kent.ac.uk</ce:e-address>
            </ce:author>
            <ce:author id="au0002"
                       author-id="S2352340922000038-0471307d055c7f70b2bf707d2f84a0f0"
                       orcid="0000-0002-0847-8880">
               <ce:given-name>Konstantinos</ce:given-name>
               <ce:surname>Sirlantzis</ce:surname>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/methodology">Methodology</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/validation">Validation</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/resources">Resources</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/supervision">Supervision</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/project-administration">Project administration</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/writing-review-editing">Writing – review &amp; editing</ce:contributor-role>
            </ce:author>
            <ce:author id="au0003"
                       author-id="S2352340922000038-43f65d45808bbfa91282120fac62b47e"
                       orcid="0000-0001-5590-0880">
               <ce:given-name>Gareth</ce:given-name>
               <ce:surname>Howells</ce:surname>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/writing-review-editing">Writing – review &amp; editing</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/supervision">Supervision</ce:contributor-role>
               <ce:contributor-role role="http://credit.niso.org/contributor-roles/methodology">Methodology</ce:contributor-role>
            </ce:author>
            <ce:affiliation id="aff0001"
                            affiliation-id="S2352340922000038-bd931831de27063ddbe32fbb87527c50">
               <ce:textfn id="cetextfn0001">School of Engineering, University of Kent, Canterbury, UK</ce:textfn>
               <sa:affiliation>
                  <sa:organization>School of Engineering</sa:organization>
                  <sa:organization>University of Kent</sa:organization>
                  <sa:city>Canterbury</sa:city>
                  <sa:country>UK</sa:country>
               </sa:affiliation>
               <ce:source-text id="staff0001">School of Engineering, University of Kent, Canterbury, UK</ce:source-text>
            </ce:affiliation>
            <ce:correspondence id="cor0001">
               <ce:label>⁎</ce:label>
               <ce:text id="cetext0001">Corresponding author.</ce:text>
            </ce:correspondence>
         </ce:author-group>
         <ce:date-received day="14" month="9" year="2021"/>
         <ce:date-revised day="6" month="12" year="2021"/>
         <ce:date-accepted day="4" month="1" year="2022"/>
         <ce:abstract id="abs0001" view="all" class="author">
            <ce:section-title id="cesectitle0001">Abstract</ce:section-title>
            <ce:abstract-sec id="abss0001" view="all">
               <ce:simple-para id="spara006" view="all">The purpose of the dataset is to provide annotated images for pixel classification tasks with application to powered wheelchair users. As some of the widely available datasets contain only general objects, we introduced this dataset to cover the missing pieces, which can be considered as application-specific objects. However, these objects of interest are not only important for powered wheelchair users but also for indoor navigation and environmental understanding in general. For example, indoor assistive and service robots need to comprehend their surroundings to ease navigation and interaction with different size objects. The proposed dataset is recorded using a camera installed on a powered wheelchair. The camera is installed beneath the joystick so that it can have a clear vision with no obstructions from the user's body or legs. The powered wheelchair is then driven through the corridors of the indoor environment, and a one-minute video is recorded. The collected video is annotated on the pixel level for semantic segmentation (pixel classification) tasks. Pixels of different objects are annotated using MATLAB software. The dataset has various object sizes (small, medium, and large), which can explain the variation of the pixel's distribution in the dataset. Usually, Deep Convolutional Neural Networks (DCNNs) that perform well on large-size objects fail to produce accurate results on small-size objects. Whereas training a DCNN on a multi-size objects dataset can build more robust systems. Although the recorded objects are vital for many applications, we have included more images of different kinds of door handles with different angles, orientations, and illuminations as they are rare in the publicly available datasets. The proposed dataset has 1549 images and covers nine different classes. We used the dataset to train and test a semantic segmentation system that can aid and guide visually impaired users by providing visual cues. The dataset is made publicly available at <ce:inter-ref id="interref0001"
                                xlink:href="https://data.mendeley.com/datasets/hs5w7xfzdk/1"
                                xlink:type="simple">this link</ce:inter-ref>.</ce:simple-para>
            </ce:abstract-sec>
         </ce:abstract>
         <ce:keywords id="keys0001" view="all" class="keyword">
            <ce:section-title id="cesectitle0002">Keywords</ce:section-title>
            <ce:keyword id="key0001">
               <ce:text id="cetext0002">Semantic segmentation</ce:text>
            </ce:keyword>
            <ce:keyword id="key0002">
               <ce:text id="cetext0003">Indoor objects</ce:text>
            </ce:keyword>
            <ce:keyword id="key0003">
               <ce:text id="cetext0004">Door handles</ce:text>
            </ce:keyword>
            <ce:keyword id="key0004">
               <ce:text id="cetext0005">Image dataset</ce:text>
            </ce:keyword>
            <ce:keyword id="key0005">
               <ce:text id="cetext0006">Deep learning</ce:text>
            </ce:keyword>
            <ce:keyword id="key0006">
               <ce:text id="cetext0007">Pixels classification</ce:text>
            </ce:keyword>
            <ce:keyword id="key0007">
               <ce:text id="cetext0008">Convolutional neural network</ce:text>
            </ce:keyword>
         </ce:keywords>
      </ja:head>
   </ja:article>
</doc:document>
